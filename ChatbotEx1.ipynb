{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rushvi2/ChatbotProject/blob/main/ChatbotEx1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 0: Ensure TensorFlow 2.x is installed\n",
        "# !pip install tensorflow\n",
        "\n",
        "# Step 1: Import necessary libraries\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "kQci9JtOr1k5"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    'Hello',\n",
        "    'Hi',\n",
        "    'Goodbye',\n",
        "    'I am done',\n",
        "    'bye',\n",
        "    'How are you?',\n",
        "    'What is your name?',\n",
        "    'What is TensorFlow?',\n",
        "    'Do you like Python?',\n",
        "    'What is your favorite color?',\n",
        "    'Tell me a joke.',\n",
        "    'What time is it?',\n",
        "    'Do you know Siri?',\n",
        "    'Can you play music?',\n",
        "    'How do I make tea?',\n",
        "    'What’s the weather like?',\n",
        "    'Tell me something interesting.',\n",
        "    'Who created you?',\n",
        "    'Do you dream of electric sheep?',\n",
        "    'Can you speak other languages?',\n",
        "    'What is machine learning?',\n",
        "    'Who won the World Cup in 2018?',\n",
        "    'What can you do?',\n",
        "    'Are you intelligent?',\n",
        "    'Do you have feelings?',\n",
        "    'What is the capital of France?',\n",
        "    'Who is the president of the United States?',\n",
        "    'What is the tallest mountain in the world?',\n",
        "    'How many continents are there?',\n",
        "    'What is the population of Earth?',\n",
        "    'Who wrote Hamlet?',\n",
        "    'What is quantum computing?',\n",
        "    'What is the speed of light?',\n",
        "    'Can you solve math problems?',\n",
        "    'What is your favorite movie?',\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    'Hi there! How can I assist you today?',\n",
        "    'Hello! What can I do for you?',\n",
        "    'Goodbye! Have a nice day!',\n",
        "    'Alright, if you need anything else, just ask. Goodbye!',\n",
        "    'Goodbye! It was nice talking to you.',\n",
        "    'I am just a bot, but I am doing fine.',\n",
        "    'I am a chatbot created by OpenAI.',\n",
        "    'TensorFlow is a framework for building machine learning models.',\n",
        "    'Yes, Python is one of my favorite programming languages.',\n",
        "    'I do not have a favorite color. I am a bot.',\n",
        "    'Why don’t scientists trust atoms? Because they make up everything.',\n",
        "    'I am not sure, I do not have access to the current time.',\n",
        "    'Yes, Siri is another virtual assistant.',\n",
        "    'I cannot play music, but I can chat with you.',\n",
        "    'Boil water, then pour it over your tea. Steep for a few minutes.',\n",
        "    'I cannot check real-time data, including the weather.',\n",
        "    'The shortest war in history lasted 38 minutes.',\n",
        "    'I was created by developers using TensorFlow and Python.',\n",
        "    'I think about learning data patterns, not sheep.',\n",
        "    'Yes, but I am primarily programmed to respond in English.',\n",
        "    'Machine learning is a field of AI focused on teaching machines to learn from data.',\n",
        "    'France won the FIFA World Cup in 2018.',\n",
        "    'I can chat with you and answer questions to the best of my training.',\n",
        "    'My intelligence is artificial, designed by humans.',\n",
        "    'I do not have feelings. I process input and provide responses.',\n",
        "    'The capital of France is Paris.',\n",
        "    'As of my last update, please check the latest information online.',\n",
        "    'Mount Everest is considered the tallest mountain above sea level.',\n",
        "    'There are seven continents on Earth.',\n",
        "    'The Earth’s population is over 7 billion people.',\n",
        "    'William Shakespeare wrote Hamlet.',\n",
        "    'Quantum computing is computing using quantum-mechanical phenomena.',\n",
        "    'The speed of light is approximately 299,792 kilometers per second.',\n",
        "    'I can help solve simple math problems.',\n",
        "    'I do not watch movies, but I can discuss them based on my training data.',\n",
        "]\n"
      ],
      "metadata": {
        "id": "AWP2WEmNtSfF"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example dataset: A small set of question-answer pairs\n",
        "#questions = ['How are you?', 'What is your name?', 'What is TensorFlow?', 'Do you like Python?']\n",
        "#answers = ['I am fine.', 'I am a bot.', 'TensorFlow is a machine learning library.', 'Yes, Python is great!']\n",
        "\n",
        "# Tokenize the sentences (this is a very simplified approach)\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(questions + answers)\n",
        "sequences_questions = tokenizer.texts_to_sequences(questions)\n",
        "sequences_answers = tokenizer.texts_to_sequences(answers)\n",
        "\n",
        "# Pad the sequences\n",
        "max_length = max(max(len(seq) for seq in sequences_questions), max(len(seq) for seq in sequences_answers))\n",
        "padded_questions = tf.keras.preprocessing.sequence.pad_sequences(sequences_questions, maxlen=max_length, padding='post')\n",
        "padded_answers = tf.keras.preprocessing.sequence.pad_sequences(sequences_answers, maxlen=max_length, padding='post')\n",
        "\n",
        "# Vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n"
      ],
      "metadata": {
        "id": "QUHG4Y6Tr5F3"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model parameters\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "\n",
        "# Define the encoder model\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "encoder_embedding = Embedding(vocab_size, embedding_dim)(encoder_inputs)\n",
        "encoder_outputs, state_h, state_c = LSTM(units, return_state=True)(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Define the decoder model\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "decoder_embedding = Embedding(vocab_size, embedding_dim)(decoder_inputs)\n",
        "decoder_lstm = LSTM(units, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = Dense(vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the seq2seq model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Summary of the model\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLgHbyfQr-eR",
        "outputId": "bbb8829a-cd16-4348-dc62-77280eb467db"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_20\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_41 (InputLayer)       [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " input_42 (InputLayer)       [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " embedding_40 (Embedding)    (None, None, 256)            54272     ['input_41[0][0]']            \n",
            "                                                                                                  \n",
            " embedding_41 (Embedding)    (None, None, 256)            54272     ['input_42[0][0]']            \n",
            "                                                                                                  \n",
            " lstm_40 (LSTM)              [(None, 1024),               5246976   ['embedding_40[0][0]']        \n",
            "                              (None, 1024),                                                       \n",
            "                              (None, 1024)]                                                       \n",
            "                                                                                                  \n",
            " lstm_41 (LSTM)              [(None, None, 1024),         5246976   ['embedding_41[0][0]',        \n",
            "                              (None, 1024),                          'lstm_40[0][1]',             \n",
            "                              (None, 1024)]                          'lstm_40[0][2]']             \n",
            "                                                                                                  \n",
            " dense_20 (Dense)            (None, None, 212)            217300    ['lstm_41[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 10819796 (41.27 MB)\n",
            "Trainable params: 10819796 (41.27 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare decoder input data that just contains the start token and the rest is all 0\n",
        "# It's a simplified approach, in practice, you would use teacher forcing or another technique\n",
        "decoder_input_data = np.zeros_like(padded_answers)\n",
        "decoder_input_data[:, 0] = 1  # Assuming 1 is the start token\n",
        "\n",
        "# Train the model\n",
        "model.fit([padded_questions, decoder_input_data], np.expand_dims(padded_answers, -1), batch_size=2, epochs=200)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrf87Hl5sZnM",
        "outputId": "88534334-a0fb-46db-cbfb-7b7a07600021"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "18/18 [==============================] - 4s 44ms/step - loss: 4.0314 - accuracy: 0.3771\n",
            "Epoch 2/300\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 3.5535 - accuracy: 0.4152\n",
            "Epoch 3/300\n",
            "18/18 [==============================] - 0s 23ms/step - loss: 3.4887 - accuracy: 0.4076\n",
            "Epoch 4/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 3.5336 - accuracy: 0.4057\n",
            "Epoch 5/300\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 3.4440 - accuracy: 0.3981\n",
            "Epoch 6/300\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 3.3813 - accuracy: 0.4133\n",
            "Epoch 7/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 3.3829 - accuracy: 0.4133\n",
            "Epoch 8/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 3.2251 - accuracy: 0.4133\n",
            "Epoch 9/300\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 3.1581 - accuracy: 0.4152\n",
            "Epoch 10/300\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 3.1035 - accuracy: 0.4190\n",
            "Epoch 11/300\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 3.0476 - accuracy: 0.4343\n",
            "Epoch 12/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 3.0019 - accuracy: 0.4229\n",
            "Epoch 13/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 2.9830 - accuracy: 0.4305\n",
            "Epoch 14/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 2.9135 - accuracy: 0.4248\n",
            "Epoch 15/300\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 2.8394 - accuracy: 0.4286\n",
            "Epoch 16/300\n",
            "18/18 [==============================] - 0s 17ms/step - loss: 2.8552 - accuracy: 0.4362\n",
            "Epoch 17/300\n",
            "18/18 [==============================] - 0s 17ms/step - loss: 2.7658 - accuracy: 0.4343\n",
            "Epoch 18/300\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 2.7182 - accuracy: 0.4324\n",
            "Epoch 19/300\n",
            "18/18 [==============================] - 0s 23ms/step - loss: 2.6708 - accuracy: 0.4248\n",
            "Epoch 20/300\n",
            "18/18 [==============================] - 0s 24ms/step - loss: 2.5806 - accuracy: 0.4362\n",
            "Epoch 21/300\n",
            "18/18 [==============================] - 0s 18ms/step - loss: 2.5864 - accuracy: 0.4248\n",
            "Epoch 22/300\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 2.5440 - accuracy: 0.4305\n",
            "Epoch 23/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 2.4403 - accuracy: 0.4400\n",
            "Epoch 24/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 2.4444 - accuracy: 0.4381\n",
            "Epoch 25/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 2.3772 - accuracy: 0.4324\n",
            "Epoch 26/300\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 2.3710 - accuracy: 0.4438\n",
            "Epoch 27/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 2.4251 - accuracy: 0.4324\n",
            "Epoch 28/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 2.3229 - accuracy: 0.4324\n",
            "Epoch 29/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 2.2742 - accuracy: 0.4362\n",
            "Epoch 30/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 2.2527 - accuracy: 0.4286\n",
            "Epoch 31/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 2.2073 - accuracy: 0.4362\n",
            "Epoch 32/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 2.1704 - accuracy: 0.4476\n",
            "Epoch 33/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 2.2049 - accuracy: 0.4419\n",
            "Epoch 34/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 2.2087 - accuracy: 0.4324\n",
            "Epoch 35/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 2.1286 - accuracy: 0.4533\n",
            "Epoch 36/300\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 2.0869 - accuracy: 0.4476\n",
            "Epoch 37/300\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 2.0414 - accuracy: 0.4533\n",
            "Epoch 38/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 2.1167 - accuracy: 0.4400\n",
            "Epoch 39/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 2.1718 - accuracy: 0.4286\n",
            "Epoch 40/300\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 2.0323 - accuracy: 0.4210\n",
            "Epoch 41/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 2.0194 - accuracy: 0.4362\n",
            "Epoch 42/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 2.0182 - accuracy: 0.4362\n",
            "Epoch 43/300\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 1.9613 - accuracy: 0.4362\n",
            "Epoch 44/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 1.9978 - accuracy: 0.4419\n",
            "Epoch 45/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 1.9465 - accuracy: 0.4705\n",
            "Epoch 46/300\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 1.9371 - accuracy: 0.4590\n",
            "Epoch 47/300\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 1.8633 - accuracy: 0.4495\n",
            "Epoch 48/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 1.9412 - accuracy: 0.4514\n",
            "Epoch 49/300\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 1.9420 - accuracy: 0.4419\n",
            "Epoch 50/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 1.8235 - accuracy: 0.4667\n",
            "Epoch 51/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 1.8238 - accuracy: 0.4438\n",
            "Epoch 52/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 1.8951 - accuracy: 0.4552\n",
            "Epoch 53/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 1.8202 - accuracy: 0.4438\n",
            "Epoch 54/300\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 1.8021 - accuracy: 0.4648\n",
            "Epoch 55/300\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 1.8119 - accuracy: 0.4610\n",
            "Epoch 56/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 1.7608 - accuracy: 0.4457\n",
            "Epoch 57/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 1.8075 - accuracy: 0.4514\n",
            "Epoch 58/300\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 1.7840 - accuracy: 0.4457\n",
            "Epoch 59/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 1.6285 - accuracy: 0.4743\n",
            "Epoch 60/300\n",
            "18/18 [==============================] - 0s 18ms/step - loss: 1.6613 - accuracy: 0.4495\n",
            "Epoch 61/300\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 1.6787 - accuracy: 0.4610\n",
            "Epoch 62/300\n",
            "18/18 [==============================] - 0s 18ms/step - loss: 1.6043 - accuracy: 0.4743\n",
            "Epoch 63/300\n",
            "18/18 [==============================] - 0s 17ms/step - loss: 1.6627 - accuracy: 0.4762\n",
            "Epoch 64/300\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 1.5690 - accuracy: 0.4762\n",
            "Epoch 65/300\n",
            "18/18 [==============================] - 0s 18ms/step - loss: 1.5808 - accuracy: 0.4705\n",
            "Epoch 66/300\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 1.4892 - accuracy: 0.4667\n",
            "Epoch 67/300\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 1.5579 - accuracy: 0.4762\n",
            "Epoch 68/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 1.5327 - accuracy: 0.4667\n",
            "Epoch 69/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 1.6049 - accuracy: 0.4933\n",
            "Epoch 70/300\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 1.4709 - accuracy: 0.4819\n",
            "Epoch 71/300\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 1.5711 - accuracy: 0.4571\n",
            "Epoch 72/300\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 1.3514 - accuracy: 0.5143\n",
            "Epoch 73/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 1.4977 - accuracy: 0.4610\n",
            "Epoch 74/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 1.3277 - accuracy: 0.5010\n",
            "Epoch 75/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 1.3927 - accuracy: 0.4971\n",
            "Epoch 76/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 1.3839 - accuracy: 0.5105\n",
            "Epoch 77/300\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 1.3782 - accuracy: 0.4838\n",
            "Epoch 78/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 1.3209 - accuracy: 0.5352\n",
            "Epoch 79/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 1.2344 - accuracy: 0.5352\n",
            "Epoch 80/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 1.4306 - accuracy: 0.4667\n",
            "Epoch 81/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 1.2789 - accuracy: 0.5238\n",
            "Epoch 82/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 1.5205 - accuracy: 0.4381\n",
            "Epoch 83/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 1.1336 - accuracy: 0.5333\n",
            "Epoch 84/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 1.1699 - accuracy: 0.5086\n",
            "Epoch 85/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 1.3316 - accuracy: 0.5295\n",
            "Epoch 86/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 1.0854 - accuracy: 0.5543\n",
            "Epoch 87/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 1.1660 - accuracy: 0.5333\n",
            "Epoch 88/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 1.3164 - accuracy: 0.5105\n",
            "Epoch 89/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 1.0580 - accuracy: 0.5886\n",
            "Epoch 90/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 1.0326 - accuracy: 0.5714\n",
            "Epoch 91/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 1.2484 - accuracy: 0.5448\n",
            "Epoch 92/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 1.0397 - accuracy: 0.6095\n",
            "Epoch 93/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 1.1497 - accuracy: 0.5352\n",
            "Epoch 94/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 1.0730 - accuracy: 0.5790\n",
            "Epoch 95/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 1.0173 - accuracy: 0.5581\n",
            "Epoch 96/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 1.0458 - accuracy: 0.5790\n",
            "Epoch 97/300\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.9799 - accuracy: 0.5905\n",
            "Epoch 98/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.8669 - accuracy: 0.6343\n",
            "Epoch 99/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 1.1618 - accuracy: 0.5371\n",
            "Epoch 100/300\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.9506 - accuracy: 0.6000\n",
            "Epoch 101/300\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.7737 - accuracy: 0.6667\n",
            "Epoch 102/300\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.8575 - accuracy: 0.6476\n",
            "Epoch 103/300\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 1.0592 - accuracy: 0.6362\n",
            "Epoch 104/300\n",
            "18/18 [==============================] - 0s 17ms/step - loss: 0.8128 - accuracy: 0.6705\n",
            "Epoch 105/300\n",
            "18/18 [==============================] - 0s 17ms/step - loss: 0.9080 - accuracy: 0.6705\n",
            "Epoch 106/300\n",
            "18/18 [==============================] - 0s 18ms/step - loss: 0.7884 - accuracy: 0.7105\n",
            "Epoch 107/300\n",
            "18/18 [==============================] - 0s 18ms/step - loss: 0.8265 - accuracy: 0.6800\n",
            "Epoch 108/300\n",
            "18/18 [==============================] - 0s 17ms/step - loss: 0.9237 - accuracy: 0.6362\n",
            "Epoch 109/300\n",
            "18/18 [==============================] - 0s 18ms/step - loss: 0.7262 - accuracy: 0.6838\n",
            "Epoch 110/300\n",
            "18/18 [==============================] - 0s 18ms/step - loss: 0.9212 - accuracy: 0.6590\n",
            "Epoch 111/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.6285 - accuracy: 0.7486\n",
            "Epoch 112/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.9662 - accuracy: 0.6857\n",
            "Epoch 113/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.9158 - accuracy: 0.6876\n",
            "Epoch 114/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.8169 - accuracy: 0.6895\n",
            "Epoch 115/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.5574 - accuracy: 0.7790\n",
            "Epoch 116/300\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.6007 - accuracy: 0.7410\n",
            "Epoch 117/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.8395 - accuracy: 0.6686\n",
            "Epoch 118/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.7142 - accuracy: 0.7162\n",
            "Epoch 119/300\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.6397 - accuracy: 0.7505\n",
            "Epoch 120/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.4919 - accuracy: 0.8133\n",
            "Epoch 121/300\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.5235 - accuracy: 0.7810\n",
            "Epoch 122/300\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.6374 - accuracy: 0.7333\n",
            "Epoch 123/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.6363 - accuracy: 0.7581\n",
            "Epoch 124/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.4515 - accuracy: 0.7943\n",
            "Epoch 125/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.6391 - accuracy: 0.7448\n",
            "Epoch 126/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.4223 - accuracy: 0.8248\n",
            "Epoch 127/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.3391 - accuracy: 0.8705\n",
            "Epoch 128/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.7538 - accuracy: 0.7619\n",
            "Epoch 129/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.3822 - accuracy: 0.8343\n",
            "Epoch 130/300\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.3944 - accuracy: 0.8514\n",
            "Epoch 131/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.8222 - accuracy: 0.6990\n",
            "Epoch 132/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.4600 - accuracy: 0.8019\n",
            "Epoch 133/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.3871 - accuracy: 0.8419\n",
            "Epoch 134/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.4033 - accuracy: 0.8457\n",
            "Epoch 135/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.3796 - accuracy: 0.8400\n",
            "Epoch 136/300\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.7173 - accuracy: 0.7695\n",
            "Epoch 137/300\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.6709 - accuracy: 0.7581\n",
            "Epoch 138/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.4902 - accuracy: 0.7771\n",
            "Epoch 139/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.4394 - accuracy: 0.8476\n",
            "Epoch 140/300\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4064 - accuracy: 0.8229\n",
            "Epoch 141/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.4893 - accuracy: 0.8076\n",
            "Epoch 142/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.4413 - accuracy: 0.8362\n",
            "Epoch 143/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.3820 - accuracy: 0.8419\n",
            "Epoch 144/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.4590 - accuracy: 0.7962\n",
            "Epoch 145/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.3448 - accuracy: 0.8495\n",
            "Epoch 146/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.6144 - accuracy: 0.8210\n",
            "Epoch 147/300\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 0.5066 - accuracy: 0.8038\n",
            "Epoch 148/300\n",
            "18/18 [==============================] - 0s 18ms/step - loss: 0.3551 - accuracy: 0.8571\n",
            "Epoch 149/300\n",
            "18/18 [==============================] - 0s 18ms/step - loss: 0.3584 - accuracy: 0.8419\n",
            "Epoch 150/300\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.4110 - accuracy: 0.8419\n",
            "Epoch 151/300\n",
            "18/18 [==============================] - 0s 18ms/step - loss: 0.3422 - accuracy: 0.8629\n",
            "Epoch 152/300\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.4436 - accuracy: 0.8267\n",
            "Epoch 153/300\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3229 - accuracy: 0.8648\n",
            "Epoch 154/300\n",
            "18/18 [==============================] - 0s 18ms/step - loss: 0.2887 - accuracy: 0.8990\n",
            "Epoch 155/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.2037 - accuracy: 0.9295\n",
            "Epoch 156/300\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 0.2475 - accuracy: 0.9143\n",
            "Epoch 157/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.3405 - accuracy: 0.8800\n",
            "Epoch 158/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.4786 - accuracy: 0.8324\n",
            "Epoch 159/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.2763 - accuracy: 0.8743\n",
            "Epoch 160/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.3593 - accuracy: 0.8438\n",
            "Epoch 161/300\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 0.1788 - accuracy: 0.9352\n",
            "Epoch 162/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.2339 - accuracy: 0.9010\n",
            "Epoch 163/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.1457 - accuracy: 0.9410\n",
            "Epoch 164/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.1512 - accuracy: 0.9219\n",
            "Epoch 165/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.2114 - accuracy: 0.9429\n",
            "Epoch 166/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.1634 - accuracy: 0.9295\n",
            "Epoch 167/300\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 0.2088 - accuracy: 0.9257\n",
            "Epoch 168/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.1766 - accuracy: 0.9314\n",
            "Epoch 169/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.2800 - accuracy: 0.8933\n",
            "Epoch 170/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.5819 - accuracy: 0.7790\n",
            "Epoch 171/300\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 0.1535 - accuracy: 0.9410\n",
            "Epoch 172/300\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 0.1081 - accuracy: 0.9448\n",
            "Epoch 173/300\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 0.1027 - accuracy: 0.9638\n",
            "Epoch 174/300\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 0.1025 - accuracy: 0.9505\n",
            "Epoch 175/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.1028 - accuracy: 0.9486\n",
            "Epoch 176/300\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.1003 - accuracy: 0.9448\n",
            "Epoch 177/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.0953 - accuracy: 0.9657\n",
            "Epoch 178/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.0941 - accuracy: 0.9581\n",
            "Epoch 179/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.0995 - accuracy: 0.9600\n",
            "Epoch 180/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.0879 - accuracy: 0.9486\n",
            "Epoch 181/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.0977 - accuracy: 0.9448\n",
            "Epoch 182/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.0915 - accuracy: 0.9467\n",
            "Epoch 183/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.0927 - accuracy: 0.9638\n",
            "Epoch 184/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.0823 - accuracy: 0.9676\n",
            "Epoch 185/300\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.0849 - accuracy: 0.9638\n",
            "Epoch 186/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.1402 - accuracy: 0.9505\n",
            "Epoch 187/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.2401 - accuracy: 0.9295\n",
            "Epoch 188/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.2148 - accuracy: 0.9467\n",
            "Epoch 189/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.0784 - accuracy: 0.9638\n",
            "Epoch 190/300\n",
            "18/18 [==============================] - 0s 17ms/step - loss: 0.0822 - accuracy: 0.9676\n",
            "Epoch 191/300\n",
            "18/18 [==============================] - 0s 18ms/step - loss: 0.0802 - accuracy: 0.9752\n",
            "Epoch 192/300\n",
            "18/18 [==============================] - 0s 18ms/step - loss: 0.1723 - accuracy: 0.9467\n",
            "Epoch 193/300\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.5145 - accuracy: 0.8324\n",
            "Epoch 194/300\n",
            "18/18 [==============================] - 0s 18ms/step - loss: 0.5268 - accuracy: 0.9029\n",
            "Epoch 195/300\n",
            "18/18 [==============================] - 0s 18ms/step - loss: 0.2340 - accuracy: 0.9333\n",
            "Epoch 196/300\n",
            "18/18 [==============================] - 0s 18ms/step - loss: 0.3093 - accuracy: 0.9086\n",
            "Epoch 197/300\n",
            "18/18 [==============================] - 0s 18ms/step - loss: 0.1630 - accuracy: 0.9410\n",
            "Epoch 198/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.1266 - accuracy: 0.9390\n",
            "Epoch 199/300\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 0.0812 - accuracy: 0.9600\n",
            "Epoch 200/300\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 0.0906 - accuracy: 0.9714\n",
            "Epoch 201/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.0833 - accuracy: 0.9619\n",
            "Epoch 202/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.1604 - accuracy: 0.9410\n",
            "Epoch 203/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.1953 - accuracy: 0.9467\n",
            "Epoch 204/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.1070 - accuracy: 0.9771\n",
            "Epoch 205/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.0447 - accuracy: 0.9810\n",
            "Epoch 206/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.0455 - accuracy: 0.9657\n",
            "Epoch 207/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.0288 - accuracy: 0.9829\n",
            "Epoch 208/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.0334 - accuracy: 0.9848\n",
            "Epoch 209/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.1166 - accuracy: 0.9543\n",
            "Epoch 210/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.0382 - accuracy: 0.9886\n",
            "Epoch 211/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.0358 - accuracy: 0.9829\n",
            "Epoch 212/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.0545 - accuracy: 0.9752\n",
            "Epoch 213/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.0170 - accuracy: 0.9981\n",
            "Epoch 214/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.0143 - accuracy: 1.0000\n",
            "Epoch 215/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.0075 - accuracy: 1.0000\n",
            "Epoch 216/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.0061 - accuracy: 1.0000\n",
            "Epoch 217/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.0044 - accuracy: 1.0000\n",
            "Epoch 218/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.0041 - accuracy: 1.0000\n",
            "Epoch 219/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.0035 - accuracy: 1.0000\n",
            "Epoch 220/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.0030 - accuracy: 1.0000\n",
            "Epoch 221/300\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 222/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 223/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 224/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 225/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 226/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 227/300\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 228/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 229/300\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 230/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 231/300\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 232/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 233/300\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 234/300\n",
            "18/18 [==============================] - 0s 18ms/step - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 235/300\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 236/300\n",
            "18/18 [==============================] - 0s 17ms/step - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 237/300\n",
            "18/18 [==============================] - 0s 18ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 238/300\n",
            "18/18 [==============================] - 0s 18ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 239/300\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 240/300\n",
            "18/18 [==============================] - 0s 18ms/step - loss: 0.0010 - accuracy: 1.0000\n",
            "Epoch 241/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.0010 - accuracy: 1.0000\n",
            "Epoch 242/300\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 9.7524e-04 - accuracy: 1.0000\n",
            "Epoch 243/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 9.5106e-04 - accuracy: 1.0000\n",
            "Epoch 244/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 9.2860e-04 - accuracy: 1.0000\n",
            "Epoch 245/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 9.0792e-04 - accuracy: 1.0000\n",
            "Epoch 246/300\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 8.8823e-04 - accuracy: 1.0000\n",
            "Epoch 247/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 8.6899e-04 - accuracy: 1.0000\n",
            "Epoch 248/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 8.5106e-04 - accuracy: 1.0000\n",
            "Epoch 249/300\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 8.3275e-04 - accuracy: 1.0000\n",
            "Epoch 250/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 8.1618e-04 - accuracy: 1.0000\n",
            "Epoch 251/300\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 8.0101e-04 - accuracy: 1.0000\n",
            "Epoch 252/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 7.8610e-04 - accuracy: 1.0000\n",
            "Epoch 253/300\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 7.7150e-04 - accuracy: 1.0000\n",
            "Epoch 254/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 7.5657e-04 - accuracy: 1.0000\n",
            "Epoch 255/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 7.4374e-04 - accuracy: 1.0000\n",
            "Epoch 256/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 7.3171e-04 - accuracy: 1.0000\n",
            "Epoch 257/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 7.1955e-04 - accuracy: 1.0000\n",
            "Epoch 258/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 7.0725e-04 - accuracy: 1.0000\n",
            "Epoch 259/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 6.9595e-04 - accuracy: 1.0000\n",
            "Epoch 260/300\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 6.8498e-04 - accuracy: 1.0000\n",
            "Epoch 261/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 6.7415e-04 - accuracy: 1.0000\n",
            "Epoch 262/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 6.6396e-04 - accuracy: 1.0000\n",
            "Epoch 263/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 6.5343e-04 - accuracy: 1.0000\n",
            "Epoch 264/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 6.4381e-04 - accuracy: 1.0000\n",
            "Epoch 265/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 6.3356e-04 - accuracy: 1.0000\n",
            "Epoch 266/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 6.2546e-04 - accuracy: 1.0000\n",
            "Epoch 267/300\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 6.1686e-04 - accuracy: 1.0000\n",
            "Epoch 268/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 6.0859e-04 - accuracy: 1.0000\n",
            "Epoch 269/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 5.9985e-04 - accuracy: 1.0000\n",
            "Epoch 270/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 5.9181e-04 - accuracy: 1.0000\n",
            "Epoch 271/300\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 5.8397e-04 - accuracy: 1.0000\n",
            "Epoch 272/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 5.7724e-04 - accuracy: 1.0000\n",
            "Epoch 273/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 5.6982e-04 - accuracy: 1.0000\n",
            "Epoch 274/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 5.6259e-04 - accuracy: 1.0000\n",
            "Epoch 275/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 5.5505e-04 - accuracy: 1.0000\n",
            "Epoch 276/300\n",
            "18/18 [==============================] - 0s 17ms/step - loss: 5.4852e-04 - accuracy: 1.0000\n",
            "Epoch 277/300\n",
            "18/18 [==============================] - 0s 18ms/step - loss: 5.4205e-04 - accuracy: 1.0000\n",
            "Epoch 278/300\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 5.3567e-04 - accuracy: 1.0000\n",
            "Epoch 279/300\n",
            "18/18 [==============================] - 0s 18ms/step - loss: 5.2951e-04 - accuracy: 1.0000\n",
            "Epoch 280/300\n",
            "18/18 [==============================] - 0s 18ms/step - loss: 5.2315e-04 - accuracy: 1.0000\n",
            "Epoch 281/300\n",
            "18/18 [==============================] - 0s 18ms/step - loss: 5.1722e-04 - accuracy: 1.0000\n",
            "Epoch 282/300\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 5.1172e-04 - accuracy: 1.0000\n",
            "Epoch 283/300\n",
            "18/18 [==============================] - 0s 18ms/step - loss: 5.0606e-04 - accuracy: 1.0000\n",
            "Epoch 284/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 5.0044e-04 - accuracy: 1.0000\n",
            "Epoch 285/300\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 4.9506e-04 - accuracy: 1.0000\n",
            "Epoch 286/300\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 4.8974e-04 - accuracy: 1.0000\n",
            "Epoch 287/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 4.8447e-04 - accuracy: 1.0000\n",
            "Epoch 288/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 4.7892e-04 - accuracy: 1.0000\n",
            "Epoch 289/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 4.7433e-04 - accuracy: 1.0000\n",
            "Epoch 290/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 4.6916e-04 - accuracy: 1.0000\n",
            "Epoch 291/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 4.6453e-04 - accuracy: 1.0000\n",
            "Epoch 292/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 4.6021e-04 - accuracy: 1.0000\n",
            "Epoch 293/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 4.5546e-04 - accuracy: 1.0000\n",
            "Epoch 294/300\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 4.5095e-04 - accuracy: 1.0000\n",
            "Epoch 295/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 4.4604e-04 - accuracy: 1.0000\n",
            "Epoch 296/300\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 4.4196e-04 - accuracy: 1.0000\n",
            "Epoch 297/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 4.3784e-04 - accuracy: 1.0000\n",
            "Epoch 298/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 4.3375e-04 - accuracy: 1.0000\n",
            "Epoch 299/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 4.2979e-04 - accuracy: 1.0000\n",
            "Epoch 300/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 4.2579e-04 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c9a4beb6b90>"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Implement the chat function\n",
        "def preprocess_input_text(input_text):\n",
        "    sequence = tokenizer.texts_to_sequences([input_text])\n",
        "    padded_sequence = tf.keras.preprocessing.sequence.pad_sequences(sequence, maxlen=max_length, padding='post')\n",
        "    return padded_sequence\n",
        "\n",
        "def generate_response(input_sequence):\n",
        "    response_sequence = np.zeros((1, max_length))\n",
        "    response_sequence[0, 0] = 1  # start token\n",
        "    for i in range(1, max_length):\n",
        "        prediction = model.predict([input_sequence, response_sequence]).argmax(axis=2)\n",
        "        response_sequence[0, i] = prediction[0, i-1]\n",
        "        if prediction[0, i-1] == 2:  # end token\n",
        "            break\n",
        "    return response_sequence\n",
        "\n",
        "def sequence_to_text(sequence):\n",
        "    return ' '.join(tokenizer.index_word.get(i, '') for i in sequence if i > 2)\n",
        "\n",
        "def chat_with_bot(input_text):\n",
        "    input_sequence = preprocess_input_text(input_text)\n",
        "    response_sequence = generate_response(input_sequence)\n",
        "    response_text = sequence_to_text(response_sequence[0])\n",
        "    return response_text\n",
        "\n",
        "# Step 6: Chat with the bot\n",
        "input_text = \"What is TensorFlow?\"\n",
        "print(f\"You: {input_text}\")\n",
        "print(f\"Bot: {chat_with_bot(input_text)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YTq6HyUtV-D",
        "outputId": "6a731072-8583-435c-90be-fcac0335fc4b"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: What is TensorFlow?\n",
            "1/1 [==============================] - 1s 753ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Bot: tensorflow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Interactive chat with the bot\n",
        "print(\"Start chatting with the bot! Type 'quit' to exit.\")\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == 'quit' or user_input.lower() == 'Goodbye' or user_input.lower() == \"I'm done\":\n",
        "        break\n",
        "    response = chat_with_bot(user_input)\n",
        "    print(f\"Bot: {response}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPIE1rsltpcw",
        "outputId": "d7ba0855-b040-4341-8b54-906a0a9b7fa8"
      },
      "execution_count": 126,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start chatting with the bot! Type 'quit' to exit.\n",
            "You: hello\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Bot: hi there how can assist you today\n",
            "You: Do you have feelings?\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Bot: do not have feelings process input and provide responses\n",
            "You: Do you like python?\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Bot: yes python\n",
            "You: Tell me a joke\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Bot: why don’t scientists trust atoms because they make up everything\n",
            "You: How do I make tea\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Bot: boil water then pour it over your tea steep for a few minutes\n",
            "You: Who wrote hamlet\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Bot: william shakespeare wrote hamlet\n",
            "You: bye\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Bot: goodbye it was nice talking to you\n",
            "You: bye\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Bot: goodbye it was nice talking to you\n",
            "You: see you later\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Bot: do not have a favorite color am a bot\n",
            "You: what time is it\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "Bot: am not sure do not have access to the current time\n",
            "You: hello\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Bot: hi there how can assist you today\n",
            "You: quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#More traiing to paste above\n",
        "questions = [\n",
        "    'How are you?',\n",
        "    'What is your name?',\n",
        "    'What is TensorFlow?',\n",
        "    'Do you like Python?',\n",
        "    'What is your favorite color?',\n",
        "    'Tell me a joke.',\n",
        "    'What time is it?',\n",
        "    'Do you know Siri?',\n",
        "    'Can you play music?',\n",
        "    'How do I make tea?',\n",
        "    'What’s the weather like?',\n",
        "    'Tell me something interesting.',\n",
        "    'Who created you?',\n",
        "    'Do you dream of electric sheep?',\n",
        "    'Can you speak other languages?',\n",
        "    'What is machine learning?',\n",
        "    'Who won the World Cup in 2018?',\n",
        "    'What can you do?',\n",
        "    'Are you intelligent?',\n",
        "    'Do you have feelings?',\n",
        "    'What is the capital of France?',\n",
        "    'Who is the president of the United States?',\n",
        "    'What is the tallest mountain in the world?',\n",
        "    'How many continents are there?',\n",
        "    'What is the population of Earth?',\n",
        "    'Who wrote Hamlet?',\n",
        "    'What is quantum computing?',\n",
        "    'What is the speed of light?',\n",
        "    'Can you solve math problems?',\n",
        "    'What is your favorite movie?',\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    'I am just a bot, but I am doing fine.',\n",
        "    'I am a chatbot created by OpenAI.',\n",
        "    'TensorFlow is a framework for building machine learning models.',\n",
        "    'Yes, Python is one of my favorite programming languages.',\n",
        "    'I do not have a favorite color. I am a bot.',\n",
        "    'Why don’t scientists trust atoms? Because they make up everything.',\n",
        "    'I am not sure, I do not have access to the current time.',\n",
        "    'Yes, Siri is another virtual assistant.',\n",
        "    'I cannot play music, but I can chat with you.',\n",
        "    'Boil water, then pour it over your tea. Steep for a few minutes.',\n",
        "    'I cannot check real-time data, including the weather.',\n",
        "    'The shortest war in history lasted 38 minutes.',\n",
        "    'I was created by developers using TensorFlow and Python.',\n",
        "    'I think about learning data patterns, not sheep.',\n",
        "    'Yes, but I am primarily programmed to respond in English.',\n",
        "    'Machine learning is a field of AI focused on teaching machines to learn from data.',\n",
        "    'France won the FIFA World Cup in 2018.',\n",
        "    'I can chat with you and answer questions to the best of my training.',\n",
        "    'My intelligence is artificial, designed by humans.',\n",
        "    'I do not have feelings. I process input and provide responses.',\n",
        "    'The capital of France is Paris.',\n",
        "    'As of my last update, please check the latest information online.',\n",
        "    'Mount Everest is considered the tallest mountain above sea level.',\n",
        "    'There are seven continents on Earth.',\n",
        "    'The Earth’s population is over 7 billion people.',\n",
        "    'William Shakespeare wrote Hamlet.',\n",
        "    'Quantum computing is computing using quantum-mechanical phenomena.',\n",
        "    'The speed of light is approximately 299,792 kilometers per second.',\n",
        "    'I can help solve simple math problems.',\n",
        "    'I do not watch movies, but I can discuss them based on my training data.',\n",
        "]\n",
        "\n",
        "# Remember to tokenize, sequence, and pad these new data points if integrating into the previous example\n"
      ],
      "metadata": {
        "id": "GUU0ja8ys-hD"
      },
      "execution_count": 127,
      "outputs": []
    }
  ]
}